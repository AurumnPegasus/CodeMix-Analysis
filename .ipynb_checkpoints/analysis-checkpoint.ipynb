{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief overview of data-structures\n",
    "\n",
    "### For all purposes\n",
    "```python\n",
    "data = [sentences]\n",
    "sentence = [elements]\n",
    "element = {\n",
    "    'word': bruh,\n",
    "    'lang': en,\n",
    "    'pos': X\n",
    "}\n",
    "```\n",
    "\n",
    "- words: the \"words\" of the data\n",
    "- lang: e, h, b, t, n\n",
    "    - e: english\n",
    "    - h: hindi\n",
    "    - b: bengali\n",
    "    - t: tamil\n",
    "    - n: naaahh (rest)\n",
    "- pos: the part of speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hi_en_dataset.json') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for key, value in json_data.items():\n",
    "    temp = value['sentence']\n",
    "    if len(temp) >= 10:\n",
    "        data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammify(data, n, item):\n",
    "    \"\"\"\n",
    "    Builds the gram_data dictionary\n",
    "    inputs:\n",
    "        data: a list of sentences\n",
    "        sentences: a list of dict containing all the information\n",
    "        n: the n in n-gram\n",
    "        item: the dictionary key\n",
    "    outputs:\n",
    "        gram_data:\n",
    "            a list of tuples of (value, key) pairs.\n",
    "            value as the frequency of the gram,\n",
    "            given by the key\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for sentence in data:\n",
    "        x = len(sentence)\n",
    "        for i in range(x-n+1):\n",
    "            window = sentence[i:i+n]\n",
    "            key = \"\"\n",
    "            for element in window:\n",
    "                key += element[item] + \" \"\n",
    "\n",
    "            if key in d:\n",
    "                d[key] += 1\n",
    "            else:\n",
    "                d[key] = 1\n",
    "\n",
    "    d = [(v, k) for k, v in d.items()]\n",
    "    return sorted(d, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gram_data(gram_data, language, gram, item):\n",
    "    \"\"\"\n",
    "    Plots the gram data.\n",
    "    inputs:\n",
    "        gram_data:\n",
    "            a list of tuples of (value, key) pairs.\n",
    "            value as the frequency of the gram,\n",
    "            given by the key\n",
    "        \n",
    "        language:\n",
    "            the name of the data in concern\n",
    "        gram:\n",
    "            the n of the data in concern\n",
    "        data:\n",
    "            the item in concern\n",
    "    \"\"\"\n",
    "    n = len(gram_data)\n",
    "    labels = [item[1] for item in gram_data]\n",
    "    heights = [item[0] for item in gram_data]\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(x=np.arange(n), height=heights, tick_label=labels)\n",
    "    plt.title(f\"{language}: for n={gram} and parameter={item}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gram(data, query, item, is_regex):\n",
    "    \"\"\"\n",
    "    Finds the requested query in the data. \n",
    "    Could be plain search or regex search.\n",
    "    Helpful for getting samples for analysis.\n",
    "    \n",
    "    inputs:\n",
    "        data:\n",
    "            the data where the query is searched\n",
    "        query:\n",
    "            the request on what to search. space separated for bigger n\n",
    "        item:\n",
    "            the item to be searched for. could be 'token', 'pos', 'lang'    \n",
    "        is_regex: \n",
    "            if the query is regex or not\n",
    "    outputs:\n",
    "        returns a list of all the sentences containing the query\n",
    "    \"\"\"\n",
    "    # stores the final results of the data\n",
    "    query_results = []\n",
    "\n",
    "    focused_sentences = []\n",
    "    for sentence in data:\n",
    "        extracted_string = \"\"\n",
    "        for element in sentence:\n",
    "            extracted_string += element[item] + \" \"\n",
    "        focused_sentences.append((sentence, extracted_string[:-1]))\n",
    "    print(\"done reformatting the data\")\n",
    "\n",
    "    if is_regex:\n",
    "        pattern = regex.compile(query)\n",
    "        query_results = [\n",
    "            sentence for sentence, extracted_string in focused_sentences \n",
    "            if len(pattern.findall(extracted_string))\n",
    "        ]\n",
    "    else:\n",
    "        query_results = [\n",
    "            sentence for sentence, extracted_string in focused_sentences\n",
    "            if query in extracted_string\n",
    "        ]\n",
    "    print(f\"Found {len(query_results)} matches.\")\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extracted_data(data, item=\"word\"):\n",
    "    \"\"\"\n",
    "    Shows what you want to see\n",
    "    inputs:\n",
    "        data:\n",
    "            the entire data (possibly extraced one with regex)\n",
    "        item:\n",
    "            the item to print out. default is word\n",
    "    outputs:\n",
    "        a list of sentences which have tokens as `items`\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        temp = f\"{i}. \"\n",
    "        for element in sentence:\n",
    "            temp += element['word'] + \"/\" + element['pos'] + \" \"\n",
    "        extracted_data.append(temp[:-1])\n",
    "    print(f\"Length of extracted data: {len(extracted_data)}\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram = 2\n",
    "# item = 'pos'\n",
    "# plot_gram_data(grammify(data, gram, item)[:20], language='codemix', gram=gram, item=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_gram(data, query=\"what\", item=\"word\", is_regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level1_data = find_gram(data, query=\"\", item=\"pos\", is_regex=True)\n",
    "# level2_data = find_gram(level1_data, query=\"(kind)\", item=\"word\", is_regex=True)\n",
    "# get_extracted_data(level1_data, item=\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_extracted_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
